{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNModels.Models import *\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset_name = \"Cora\"\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Planetoid', name=dataset_name, transform=NormalizeFeatures())\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9467, Validation loss: 1.9455\n",
      "Test Accuracy: 0.8080\n"
     ]
    }
   ],
   "source": [
    "# This is temporary model training, will be replaced with improting pretrained model, having problems with it currently\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "\n",
    "model = GCN(hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss, val_loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc, out\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss, val_loss = train()\n",
    "    if epoch%200 == 0:\n",
    "          print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Validation loss: {val_loss:.4f}')\n",
    "\n",
    "test_acc, out = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd GraphXAI-main\n",
    "# !pip install -e .\n",
    "\n",
    "from graphxai.explainers import GNNExplainer, PGExplainer, IntegratedGradExplainer, PGMExplainer\n",
    "\n",
    "# the ones below we want to use from different libraries\n",
    "from graphxai.explainers import GNN_LRP, CAM\n",
    "\n",
    "# need to also use subgraph x from DIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 653.05it/s]\n",
      "140it [00:00, 330.41it/s]\n",
      "140it [00:00, 495.24it/s]\n",
      "140it [00:00, 517.41it/s]\n",
      "140it [00:00, 425.18it/s]\n",
      "140it [00:00, 490.66it/s]\n",
      "140it [00:00, 379.95it/s]\n",
      "140it [00:00, 464.86it/s]\n",
      "140it [00:00, 454.31it/s]\n",
      "140it [00:00, 470.79it/s]\n",
      "140it [00:00, 476.03it/s]\n",
      "140it [00:00, 303.48it/s]\n",
      "140it [00:00, 460.86it/s]\n",
      "140it [00:00, 508.38it/s]\n",
      "140it [00:00, 370.67it/s]\n",
      "140it [00:00, 412.32it/s]\n",
      "140it [00:00, 504.03it/s]\n",
      "140it [00:00, 391.87it/s]\n",
      "140it [00:00, 438.05it/s]\n",
      "140it [00:00, 497.66it/s]\n",
      "140it [00:00, 549.34it/s]\n",
      "140it [00:00, 457.54it/s]\n",
      "140it [00:00, 481.60it/s]\n",
      "140it [00:00, 483.72it/s]\n",
      "140it [00:00, 271.37it/s]\n",
      "140it [00:00, 412.29it/s]\n",
      "140it [00:00, 539.95it/s]\n",
      "140it [00:00, 563.08it/s]\n",
      "140it [00:00, 531.17it/s]\n",
      "140it [00:00, 555.29it/s]\n",
      "140it [00:00, 582.56it/s]\n",
      "140it [00:00, 603.93it/s]\n",
      "140it [00:00, 557.84it/s]\n",
      "140it [00:00, 353.83it/s]\n",
      "140it [00:00, 563.04it/s]\n",
      "140it [00:00, 529.90it/s]\n",
      "140it [00:00, 588.11it/s]\n",
      "140it [00:00, 506.21it/s]\n",
      "140it [00:00, 445.83it/s]\n",
      "140it [00:00, 368.22it/s]\n",
      "140it [00:00, 510.50it/s]\n",
      "140it [00:00, 228.41it/s]\n",
      "140it [00:00, 423.42it/s]\n",
      "140it [00:00, 419.37it/s]\n",
      "140it [00:00, 566.67it/s]\n",
      "140it [00:00, 413.42it/s]\n",
      "140it [00:00, 298.24it/s]\n",
      "140it [00:00, 515.86it/s]\n",
      "140it [00:00, 419.49it/s]\n",
      "140it [00:00, 466.02it/s]\n",
      "140it [00:00, 479.13it/s]\n",
      "140it [00:00, 526.41it/s]\n",
      "140it [00:00, 418.27it/s]\n",
      "140it [00:00, 409.20it/s]\n",
      "140it [00:00, 407.14it/s]\n",
      "140it [00:00, 409.59it/s]\n",
      "140it [00:00, 359.72it/s]\n",
      "140it [00:00, 434.84it/s]\n",
      "140it [00:00, 451.56it/s]\n",
      "140it [00:00, 550.79it/s]\n",
      "140it [00:00, 609.96it/s]\n",
      "140it [00:00, 504.41it/s]\n",
      "140it [00:00, 443.41it/s]\n",
      "140it [00:00, 453.72it/s]\n",
      "140it [00:00, 424.82it/s]\n",
      "140it [00:00, 482.66it/s]\n",
      "140it [00:00, 553.35it/s]\n",
      "140it [00:00, 559.15it/s]\n",
      "140it [00:00, 395.15it/s]\n",
      "140it [00:00, 466.20it/s]\n",
      "140it [00:00, 481.90it/s]\n",
      "140it [00:00, 381.04it/s]\n",
      "140it [00:00, 414.68it/s]\n",
      "140it [00:00, 561.53it/s]\n",
      "140it [00:00, 652.92it/s]\n",
      "140it [00:00, 380.95it/s]\n",
      "140it [00:00, 516.81it/s]\n",
      "140it [00:00, 453.86it/s]\n",
      "140it [00:00, 374.74it/s]\n",
      "140it [00:00, 296.70it/s]\n",
      "140it [00:00, 484.25it/s]\n",
      "140it [00:00, 355.00it/s]\n",
      "140it [00:00, 199.45it/s]\n",
      "140it [00:00, 306.13it/s]\n",
      "140it [00:00, 405.83it/s]\n",
      "140it [00:00, 438.94it/s]\n",
      "140it [00:00, 514.56it/s]\n",
      "140it [00:00, 564.11it/s]\n",
      "140it [00:00, 553.64it/s]\n",
      "140it [00:00, 441.26it/s]\n",
      "140it [00:01, 120.33it/s]\n",
      "140it [00:00, 174.96it/s]\n",
      "140it [00:00, 209.98it/s]\n",
      "140it [00:00, 224.63it/s]\n",
      "140it [00:00, 444.23it/s]\n",
      "140it [00:00, 580.33it/s]\n",
      "140it [00:00, 384.46it/s]\n",
      "140it [00:00, 583.56it/s]\n",
      "140it [00:00, 567.41it/s]\n",
      "140it [00:00, 585.77it/s]\n",
      "140it [00:00, 570.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time is 33.788s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GNN Explainer - discrete mask of node imp, soft mask of edge imp\n",
    "\n",
    "gnnexp = GNNExplainer(model)\n",
    "\n",
    "def gnn_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = gnnexp.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "\n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "# PGE Explainer - discrete maks of node imp, discrete mask of edge imp\n",
    "\n",
    "# needs name of emb layer of the model\n",
    "pgex = PGExplainer(model, emb_layer_name = 'conv2',  max_epochs = 100, lr = 0.1)\n",
    "pgex.train_explanation_model(data)\n",
    "\n",
    "def pge_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = pgex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "\n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated gradients - soft mask of edge imp\n",
    "\n",
    "igex = IntegratedGradExplainer(model, criterion=criterion)\n",
    "\n",
    "def ig_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = igex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "# PGME Explainer - discrete mask of node imp, randomised, can get ranking as well by asking for top 1 then 2 and so on\n",
    "\n",
    "pgm = PGMExplainer(model, explain_graph=False)\n",
    "\n",
    "def pgm_imp_nodes(node_idx, top = None):\n",
    "\n",
    "    np.random.seed(1998)\n",
    "\n",
    "    if top == None:\n",
    "\n",
    "        node_exp = pgm.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    else: \n",
    "\n",
    "        node_exp = pgm.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, top_k_nodes=top)\n",
    "\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates jacard similarity of 2 lists\n",
    "\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 4\n",
      "True Class : 4\n"
     ]
    }
   ],
   "source": [
    "out = model(data.x, data.edge_index)\n",
    "\n",
    "node_idx = 1\n",
    "\n",
    "print(\"Prediction : {}\".format(out[node_idx].argmax()))\n",
    "print(\"True Class : {}\".format(data.y[node_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1986]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 332, 654, 1454, 1986]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pge_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cb833ce684443192d0f6840795eca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 332, 470, 652, 654, 1454, 1666, 1986]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgm_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('FML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b04751f1bebd7a65ad7c5d0f9cebac4b04a61a19229941eef761806182097f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
