{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNModels.Models import *\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset_name = \"Cora\"\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Planetoid', name=dataset_name, transform=NormalizeFeatures())\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9467, Validation loss: 1.9455\n",
      "Test Accuracy: 0.8080\n"
     ]
    }
   ],
   "source": [
    "# This is temporary model training, will be replaced with improting pretrained model, having problems with it currently\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "\n",
    "model = GCN(hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss, val_loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc, out\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss, val_loss = train()\n",
    "    if epoch%200 == 0:\n",
    "          print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Validation loss: {val_loss:.4f}')\n",
    "\n",
    "test_acc, out = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd GraphXAI-main\n",
    "# !pip install -e .\n",
    "\n",
    "from graphxai.explainers import GNNExplainer, PGExplainer, IntegratedGradExplainer, PGMExplainer\n",
    "\n",
    "# the ones below we want to use from different libraries\n",
    "from graphxai.explainers import GNN_LRP, CAM\n",
    "\n",
    "# need to also use subgraph x from DIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Explainer and PGE Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 720.82it/s]\n",
      "140it [00:00, 532.68it/s]\n",
      "140it [00:00, 572.83it/s]\n",
      "140it [00:00, 558.59it/s]\n",
      "140it [00:00, 611.44it/s]\n",
      "140it [00:00, 641.28it/s]\n",
      "140it [00:00, 329.54it/s]\n",
      "140it [00:00, 496.31it/s]\n",
      "140it [00:00, 544.37it/s]\n",
      "140it [00:00, 410.69it/s]\n",
      "140it [00:00, 479.45it/s]\n",
      "140it [00:00, 565.34it/s]\n",
      "140it [00:00, 512.06it/s]\n",
      "140it [00:00, 470.70it/s]\n",
      "140it [00:00, 439.00it/s]\n",
      "140it [00:00, 297.73it/s]\n",
      "140it [00:00, 458.47it/s]\n",
      "140it [00:00, 519.75it/s]\n",
      "140it [00:00, 555.44it/s]\n",
      "140it [00:00, 511.41it/s]\n",
      "140it [00:00, 497.85it/s]\n",
      "140it [00:00, 455.57it/s]\n",
      "140it [00:00, 457.49it/s]\n",
      "140it [00:00, 575.20it/s]\n",
      "140it [00:00, 520.68it/s]\n",
      "140it [00:00, 513.19it/s]\n",
      "140it [00:00, 497.79it/s]\n",
      "140it [00:00, 480.28it/s]\n",
      "140it [00:00, 479.42it/s]\n",
      "140it [00:00, 583.70it/s]\n",
      "140it [00:00, 594.17it/s]\n",
      "140it [00:00, 556.04it/s]\n",
      "140it [00:00, 583.64it/s]\n",
      "140it [00:00, 561.90it/s]\n",
      "140it [00:00, 503.16it/s]\n",
      "140it [00:00, 530.42it/s]\n",
      "140it [00:00, 475.23it/s]\n",
      "140it [00:00, 558.05it/s]\n",
      "140it [00:00, 601.46it/s]\n",
      "140it [00:00, 574.38it/s]\n",
      "140it [00:00, 536.63it/s]\n",
      "140it [00:00, 556.97it/s]\n",
      "140it [00:00, 509.09it/s]\n",
      "140it [00:00, 513.92it/s]\n",
      "140it [00:00, 463.59it/s]\n",
      "140it [00:00, 506.61it/s]\n",
      "140it [00:00, 480.15it/s]\n",
      "140it [00:00, 504.82it/s]\n",
      "140it [00:00, 574.99it/s]\n",
      "140it [00:00, 591.34it/s]\n",
      "140it [00:00, 541.15it/s]\n",
      "140it [00:00, 474.42it/s]\n",
      "140it [00:00, 355.10it/s]\n",
      "140it [00:00, 585.92it/s]\n",
      "140it [00:00, 421.37it/s]\n",
      "140it [00:00, 443.06it/s]\n",
      "140it [00:00, 525.14it/s]\n",
      "140it [00:00, 542.52it/s]\n",
      "140it [00:00, 570.51it/s]\n",
      "140it [00:00, 518.41it/s]\n",
      "140it [00:00, 476.16it/s]\n",
      "140it [00:00, 526.10it/s]\n",
      "140it [00:00, 551.66it/s]\n",
      "140it [00:00, 592.66it/s]\n",
      "140it [00:00, 561.39it/s]\n",
      "140it [00:00, 684.91it/s]\n",
      "140it [00:00, 530.42it/s]\n",
      "140it [00:00, 610.19it/s]\n",
      "140it [00:00, 675.32it/s]\n",
      "140it [00:00, 681.38it/s]\n",
      "140it [00:00, 644.94it/s]\n",
      "140it [00:00, 352.67it/s]\n",
      "140it [00:00, 600.75it/s]\n",
      "140it [00:00, 274.88it/s]\n",
      "140it [00:00, 545.54it/s]\n",
      "140it [00:00, 546.99it/s]\n",
      "140it [00:00, 554.56it/s]\n",
      "140it [00:00, 671.84it/s]\n",
      "140it [00:00, 696.55it/s]\n",
      "140it [00:00, 601.53it/s]\n",
      "140it [00:00, 682.77it/s]\n",
      "140it [00:00, 660.87it/s]\n",
      "140it [00:00, 509.61it/s]\n",
      "140it [00:00, 565.29it/s]\n",
      "140it [00:00, 604.74it/s]\n",
      "140it [00:00, 613.27it/s]\n",
      "140it [00:00, 483.57it/s]\n",
      "140it [00:00, 591.59it/s]\n",
      "140it [00:00, 659.70it/s]\n",
      "140it [00:00, 669.70it/s]\n",
      "140it [00:00, 687.57it/s]\n",
      "140it [00:00, 609.86it/s]\n",
      "140it [00:00, 590.75it/s]\n",
      "140it [00:00, 680.19it/s]\n",
      "140it [00:00, 523.13it/s]\n",
      "140it [00:00, 625.54it/s]\n",
      "140it [00:00, 565.67it/s]\n",
      "140it [00:00, 594.71it/s]\n",
      "140it [00:00, 661.45it/s]\n",
      "140it [00:00, 637.87it/s]\n",
      "140it [00:00, 691.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time is 26.556s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GNN Explainer - discrete mask of node imp, soft mask of edge imp\n",
    "\n",
    "gnnexp = GNNExplainer(model)\n",
    "\n",
    "def gnn_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = gnnexp.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "\n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "# PGE Explainer - discrete maks of node imp, discrete mask of edge imp\n",
    "\n",
    "# needs name of emb layer of the model\n",
    "pgex = PGExplainer(model, emb_layer_name = 'conv2',  max_epochs = 100, lr = 0.1)\n",
    "pgex.train_explanation_model(data)\n",
    "\n",
    "def pge_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = pgex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "\n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients and PGM Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated gradients - soft mask of edge imp\n",
    "\n",
    "igex = IntegratedGradExplainer(model, criterion=criterion)\n",
    "\n",
    "def ig_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = igex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "# PGME Explainer - discrete mask of node imp, randomised, can get ranking as well by asking for top 1 then 2 and so on\n",
    "\n",
    "pgm = PGMExplainer(model, explain_graph=False)\n",
    "\n",
    "def pgm_imp_nodes(node_idx, top = None):\n",
    "\n",
    "    np.random.seed(1998)\n",
    "\n",
    "    if top == None:\n",
    "\n",
    "        node_exp = pgm.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    else: \n",
    "\n",
    "        node_exp = pgm.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, top_k_nodes=top)\n",
    "\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAM - \n",
    "\n",
    "camex = CAM(model)\n",
    "\n",
    "def cam_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = camex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "\n",
    "\n",
    "# LRP - \n",
    "\n",
    "# lrp = GNN_LRP(model)\n",
    "\n",
    "# def lrp_imp_nodes(node_idx, out):\n",
    "\n",
    "#     node_exp = lrp.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, num_classes = dataset.num_classes, label = out[node_idx].argmax())\n",
    "    \n",
    "#     imp_nodes = []\n",
    "\n",
    "#     mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "#     for k in node_exp.node_reference.keys():\n",
    "\n",
    "#         if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "#             imp_nodes.append(k)\n",
    "\n",
    "#     return imp_nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates jacard similarity of 2 lists\n",
    "\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 3\n",
      "True Class : 3\n"
     ]
    }
   ],
   "source": [
    "out = model(data.x, data.edge_index)\n",
    "\n",
    "node_idx = 15\n",
    "\n",
    "print(\"Prediction : {}\".format(out[node_idx].argmax()))\n",
    "print(\"True Class : {}\".format(data.y[node_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2177]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 395, 640, 765, 894, 1090, 1147, 1598, 2177, 2367, 2369, 2370]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pge_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffab7093a4d48c1b86b7792cbfc044a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 395,\n",
       " 599,\n",
       " 640,\n",
       " 765,\n",
       " 894,\n",
       " 1090,\n",
       " 1093,\n",
       " 1147,\n",
       " 1271,\n",
       " 1598,\n",
       " 2177,\n",
       " 2367,\n",
       " 2368,\n",
       " 2369,\n",
       " 2370,\n",
       " 2371]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgm_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 395,\n",
       " 599,\n",
       " 640,\n",
       " 765,\n",
       " 894,\n",
       " 1090,\n",
       " 1093,\n",
       " 1147,\n",
       " 1271,\n",
       " 1598,\n",
       " 2177,\n",
       " 2367,\n",
       " 2368,\n",
       " 2369,\n",
       " 2370,\n",
       " 2371]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_imp_nodes(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DIG_GXAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a96d9a9b024afc8122ecb662ff5349e8dc2ebea5f61559fdd75ed5db623e2825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
