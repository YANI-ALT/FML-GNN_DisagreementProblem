{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DIG_GXAI/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GNNModels.Models import *\n",
    "from GNNModels.Train import *\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Cora dataset of Pytorch in this example. Same code cells work if you replace Cora by PubMed or CiteSeer.\n",
    "\n",
    "**NOTE:**\n",
    "Node classification will almost always have a single graph. Hence we use dataset[0] and the training regime is also setup for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='GNNModels/data/Planetoid', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments for training require a params dictionary with settings for training. The details of the keys are below.\n",
    "\n",
    "* lr : [float] Learning Rate (Adam optimiser)\n",
    "* weight_decay : [float] weight decay during training (Adam Optimiser)\n",
    "* epochs : [int] training runs to make\n",
    "* verbose : [True,False] to print output after every 10 epochs\n",
    "* save_wts : [Path] Will store the resulting weights after training. Prefrably make it point to the checkpoints dir and use Model_dataset_epochs=int_timestamp.pt for name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.8764, Val: 0.4280, Test: 0.4760\n",
      "Epoch: 020, Loss: 1.7568, Val: 0.6180, Test: 0.6500\n",
      "Epoch: 030, Loss: 1.6102, Val: 0.7200, Test: 0.7310\n",
      "Epoch: 040, Loss: 1.3972, Val: 0.7360, Test: 0.7410\n",
      "Epoch: 050, Loss: 1.2543, Val: 0.7520, Test: 0.7900\n",
      "Epoch: 060, Loss: 1.0201, Val: 0.7540, Test: 0.7890\n",
      "Epoch: 070, Loss: 0.8818, Val: 0.7780, Test: 0.7990\n",
      "Epoch: 080, Loss: 0.8277, Val: 0.7740, Test: 0.8020\n",
      "Epoch: 090, Loss: 0.6746, Val: 0.7740, Test: 0.8070\n",
      "Epoch: 100, Loss: 0.6375, Val: 0.7840, Test: 0.8110\n",
      "Epoch: 110, Loss: 0.5905, Val: 0.7740, Test: 0.8120\n",
      "Epoch: 120, Loss: 0.5309, Val: 0.7740, Test: 0.8080\n",
      "Epoch: 130, Loss: 0.5062, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 140, Loss: 0.4919, Val: 0.7920, Test: 0.8180\n",
      "Epoch: 150, Loss: 0.4847, Val: 0.7880, Test: 0.8180\n",
      "Epoch: 160, Loss: 0.4715, Val: 0.7820, Test: 0.8140\n",
      "Epoch: 170, Loss: 0.3718, Val: 0.7900, Test: 0.8200\n",
      "Epoch: 180, Loss: 0.3631, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 190, Loss: 0.4200, Val: 0.7860, Test: 0.8220\n",
      "Epoch: 200, Loss: 0.3751, Val: 0.7880, Test: 0.8140\n",
      "Epoch: 210, Loss: 0.3711, Val: 0.7920, Test: 0.8130\n",
      "Epoch: 220, Loss: 0.3122, Val: 0.7820, Test: 0.8080\n",
      "Epoch: 230, Loss: 0.3605, Val: 0.7860, Test: 0.8110\n",
      "Epoch: 240, Loss: 0.2729, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 250, Loss: 0.3172, Val: 0.7880, Test: 0.8140\n",
      "Epoch: 260, Loss: 0.3289, Val: 0.7860, Test: 0.8130\n",
      "Epoch: 270, Loss: 0.2892, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 280, Loss: 0.2833, Val: 0.7860, Test: 0.8130\n",
      "Epoch: 290, Loss: 0.2963, Val: 0.7800, Test: 0.8100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Loss': tensor(0.3207, grad_fn=<NllLossBackward0>),\n",
       " 'Val': 0.782,\n",
       " 'Test': 0.807}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(hidden_channels=16,num_features=dataset.num_features,num_classes=dataset.num_classes)\n",
    "\n",
    "params={'lr':0.01,'weight_decay':5e-4,'epochs':300,'verbose':True, 'save_wts':''}\n",
    "# params={'lr':0.01,'weight_decay':5e-4,'epochs':300,'verbose':True,'save_wts':'checkpoints/GCN_Cora_epochs=300.pt'}\n",
    "\n",
    "TrainModel(model,dataset[0],params,'NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.8394, Val: 0.7900, Test: 0.7930\n",
      "Epoch: 020, Loss: 1.6972, Val: 0.7920, Test: 0.8090\n",
      "Epoch: 030, Loss: 1.4835, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 040, Loss: 1.2726, Val: 0.8000, Test: 0.8080\n",
      "Epoch: 050, Loss: 1.0636, Val: 0.8000, Test: 0.7980\n",
      "Epoch: 060, Loss: 0.7850, Val: 0.8000, Test: 0.8000\n",
      "Epoch: 070, Loss: 0.6366, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 080, Loss: 0.5270, Val: 0.7920, Test: 0.7900\n",
      "Epoch: 090, Loss: 0.4660, Val: 0.7900, Test: 0.7970\n",
      "Epoch: 100, Loss: 0.3983, Val: 0.7840, Test: 0.7830\n",
      "Epoch: 110, Loss: 0.3741, Val: 0.7880, Test: 0.7930\n",
      "Epoch: 120, Loss: 0.3665, Val: 0.7900, Test: 0.7900\n",
      "Epoch: 130, Loss: 0.2702, Val: 0.8000, Test: 0.7950\n",
      "Epoch: 140, Loss: 0.3322, Val: 0.7820, Test: 0.7810\n",
      "Epoch: 150, Loss: 0.2832, Val: 0.7960, Test: 0.7870\n",
      "Epoch: 160, Loss: 0.2964, Val: 0.7940, Test: 0.7940\n",
      "Epoch: 170, Loss: 0.2784, Val: 0.7960, Test: 0.7930\n",
      "Epoch: 180, Loss: 0.2857, Val: 0.7760, Test: 0.7690\n",
      "Epoch: 190, Loss: 0.2489, Val: 0.7980, Test: 0.7920\n",
      "Epoch: 200, Loss: 0.2776, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 210, Loss: 0.2053, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 220, Loss: 0.2356, Val: 0.7980, Test: 0.7830\n",
      "Epoch: 230, Loss: 0.2333, Val: 0.7900, Test: 0.7820\n",
      "Epoch: 240, Loss: 0.2177, Val: 0.7920, Test: 0.7890\n",
      "Epoch: 250, Loss: 0.2453, Val: 0.7980, Test: 0.7910\n",
      "Epoch: 260, Loss: 0.1944, Val: 0.7980, Test: 0.7860\n",
      "Epoch: 270, Loss: 0.2384, Val: 0.7960, Test: 0.7950\n",
      "Epoch: 280, Loss: 0.2012, Val: 0.8000, Test: 0.7900\n",
      "Epoch: 290, Loss: 0.1935, Val: 0.7880, Test: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Loss': tensor(0.1889, grad_fn=<NllLossBackward0>), 'Val': 0.8, 'Test': 0.787}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= GAT(hidden_channels=8, num_features=dataset.num_features,num_classes=dataset.num_classes,heads=8)\n",
    "\n",
    "params={'lr':0.005,'weight_decay':5e-4,'epochs':300,'verbose':True,'save_wts':''}\n",
    "# params={'lr':0.005,'weight_decay':5e-4,'epochs':300,'verbose':True,'save_wts':'checkpoints/GAT_PubMed_epochs=300.pt'}\n",
    "\n",
    "TrainModel(model,dataset[0],params,'NC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for training are PROTEINS and MUTAG from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TUDataset(root='GNNModels/data/TUDataset', name='PROTEINS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Acc: 0.6237, Test Acc: 0.6749\n",
      "Epoch: 020, Train Acc: 0.6837, Test Acc: 0.6704\n",
      "Epoch: 030, Train Acc: 0.7166, Test Acc: 0.7040\n",
      "Epoch: 040, Train Acc: 0.7226, Test Acc: 0.7197\n",
      "Epoch: 050, Train Acc: 0.7286, Test Acc: 0.7063\n",
      "Epoch: 060, Train Acc: 0.6942, Test Acc: 0.6704\n",
      "Epoch: 070, Train Acc: 0.7196, Test Acc: 0.7197\n",
      "Epoch: 080, Train Acc: 0.7406, Test Acc: 0.7197\n",
      "Epoch: 090, Train Acc: 0.7331, Test Acc: 0.7175\n",
      "Epoch: 100, Train Acc: 0.7361, Test Acc: 0.7175\n",
      "Epoch: 110, Train Acc: 0.7166, Test Acc: 0.6861\n",
      "Epoch: 120, Train Acc: 0.6882, Test Acc: 0.6637\n",
      "Epoch: 130, Train Acc: 0.7361, Test Acc: 0.7130\n",
      "Epoch: 140, Train Acc: 0.6927, Test Acc: 0.6592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Loss': None, 'Train': 0.7166416791604198, 'Test': 0.6838565022421524}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN_3L(hidden_channels=64,num_features=dataset.num_node_features,num_classes=dataset.num_classes)\n",
    "\n",
    "params={'lr':0.01,'epochs':150,'verbose':True,'save_wts':''}\n",
    "# params={'lr':0.01,'epochs':150,'verbose':True,'save_wts':'checkpoints/GCN_3L_PROTEINS_epochs=300.pt'}\n",
    "\n",
    "TrainModel(model,dataset,params,type='GC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Acc: 0.6867, Test Acc: 0.6547\n",
      "Epoch: 020, Train Acc: 0.7136, Test Acc: 0.6704\n",
      "Epoch: 030, Train Acc: 0.6987, Test Acc: 0.6771\n",
      "Epoch: 040, Train Acc: 0.7196, Test Acc: 0.6973\n",
      "Epoch: 050, Train Acc: 0.6927, Test Acc: 0.6457\n",
      "Epoch: 060, Train Acc: 0.7061, Test Acc: 0.7085\n",
      "Epoch: 070, Train Acc: 0.7076, Test Acc: 0.7040\n",
      "Epoch: 080, Train Acc: 0.7151, Test Acc: 0.6726\n",
      "Epoch: 090, Train Acc: 0.7181, Test Acc: 0.6973\n",
      "Epoch: 100, Train Acc: 0.7391, Test Acc: 0.6928\n",
      "Epoch: 110, Train Acc: 0.7256, Test Acc: 0.6480\n"
     ]
    }
   ],
   "source": [
    "model = GNNGraphConv(hidden_channels=64,num_features=dataset.num_node_features,num_classes=dataset.num_classes)\n",
    "\n",
    "params={'lr':0.01,'epochs':150,'verbose':True,'save_wts':''}\n",
    "# params={'lr':0.01,'epochs':300,'verbose':True,'save_wts':'checkpoints/GNNGraphConv_PROTEINS_epochs=300.pt'}\n",
    "\n",
    "\n",
    "TrainModel(model,dataset,params,type='GC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DIG_GXAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a96d9a9b024afc8122ecb662ff5349e8dc2ebea5f61559fdd75ed5db623e2825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
