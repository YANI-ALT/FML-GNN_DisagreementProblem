{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DIG_GXAI/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from GNNModels.Models import *\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset_name = \"Cora\"\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Planetoid', name=dataset_name, transform=NormalizeFeatures())\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9467, Validation loss: 1.9455\n",
      "Test Accuracy: 0.8080\n"
     ]
    }
   ],
   "source": [
    "# # This is temporary model training, will be replaced with improting pretrained model, having problems with it currently\n",
    "\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels):\n",
    "#         super().__init__()\n",
    "#         torch.manual_seed(1)\n",
    "#         self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         #x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# model = GCN(hidden_channels=16)\n",
    "\n",
    "# model = GCN(hidden_channels=32)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# def train():\n",
    "#       model.train()\n",
    "#       optimizer.zero_grad()  # Clear gradients.\n",
    "#       out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "#       loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "#       val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "#       loss.backward()  # Derive gradients.\n",
    "#       optimizer.step()  # Update parameters based on gradients.\n",
    "#       return loss, val_loss\n",
    "\n",
    "# def test():\n",
    "#       model.eval()\n",
    "#       out = model(data.x, data.edge_index)\n",
    "#       pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#       test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "#       test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "#       return test_acc, out\n",
    "\n",
    "\n",
    "# for epoch in range(200):\n",
    "#     loss, val_loss = train()\n",
    "#     if epoch%200 == 0:\n",
    "#           print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Validation loss: {val_loss:.4f}')\n",
    "\n",
    "# test_acc, out = test()\n",
    "# print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretrained model from GNNModels\n",
    "\n",
    "model=get_model_pretrained('GCN','Cora')\n",
    "criterion=torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd GrapthXAI-main\n",
    "# !pip insall -e .\n",
    "\n",
    "from graphxai.explainers import GNNExplainer, PGExplainer, IntegratedGradExplainer, PGMExplainer\n",
    "\n",
    "# the ones below we want to use from different libraries\n",
    "from graphxai.explainers import GNN_LRP, CAM\n",
    "\n",
    "# need to also use subgraph x from DIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Explainer and PGE Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 756.22it/s]\n",
      "140it [00:00, 747.14it/s]\n",
      "140it [00:00, 945.36it/s]\n",
      "140it [00:00, 904.54it/s]\n",
      "140it [00:00, 626.60it/s]\n",
      "140it [00:00, 811.79it/s]\n",
      "140it [00:00, 878.98it/s]\n",
      "140it [00:00, 881.60it/s]\n",
      "140it [00:00, 937.04it/s]\n",
      "140it [00:00, 922.92it/s]\n",
      "140it [00:00, 758.60it/s]\n",
      "140it [00:00, 765.39it/s]\n",
      "140it [00:00, 832.97it/s]\n",
      "140it [00:00, 805.57it/s]\n",
      "140it [00:00, 703.56it/s]\n",
      "140it [00:00, 788.47it/s]\n",
      "140it [00:00, 934.79it/s]\n",
      "140it [00:00, 872.82it/s]\n",
      "140it [00:00, 969.45it/s]\n",
      "140it [00:00, 950.71it/s]\n",
      "140it [00:00, 1001.72it/s]\n",
      "140it [00:00, 996.94it/s] \n",
      "140it [00:00, 989.06it/s]\n",
      "140it [00:00, 1050.84it/s]\n",
      "140it [00:00, 1051.26it/s]\n",
      "140it [00:00, 1049.12it/s]\n",
      "140it [00:00, 1040.04it/s]\n",
      "140it [00:00, 1049.68it/s]\n",
      "140it [00:00, 992.96it/s] \n",
      "140it [00:00, 750.02it/s]\n",
      "140it [00:00, 899.60it/s]\n",
      "140it [00:00, 889.25it/s]\n",
      "140it [00:00, 954.75it/s] \n",
      "140it [00:00, 897.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time is 82.619s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GNN Explainer - discrete mask of node imp, soft mask of edge imp\n",
    "\n",
    "gnnexp = GNNExplainer(model)\n",
    "\n",
    "def gnn_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = gnnexp.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "\n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "# PGE Explainer - discrete maks of node imp, discrete mask of edge imp\n",
    "\n",
    "# needs name of emb layer of the model\n",
    "pgex = PGExplainer(model, emb_layer_name = 'conv2',  max_epochs = 500, lr = 0.01)\n",
    "pgex.train_explanation_model(data)\n",
    "\n",
    "def pge_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = pgex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "\n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients and PGM Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated gradients - soft mask of edge imp\n",
    "\n",
    "igex = IntegratedGradExplainer(model, criterion=criterion)\n",
    "\n",
    "def ig_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = igex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n",
    "# PGME Explainer - discrete mask of node imp, randomised, can get ranking as well by asking for top 1 then 2 and so on\n",
    "\n",
    "pgm = PGMExplainer(model, explain_graph=False)\n",
    "\n",
    "def pgm_imp_nodes(node_idx, top = None):\n",
    "\n",
    "    np.random.seed(1998)\n",
    "\n",
    "    if top == None:\n",
    "\n",
    "        node_exp = pgm.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index)\n",
    "\n",
    "    else: \n",
    "\n",
    "        node_exp = pgm.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, top_k_nodes=top)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if node_exp.node_imp[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAM - soft mask of node importanct\n",
    "\n",
    "camex = CAM(model)\n",
    "\n",
    "def cam_imp_nodes(node_idx):\n",
    "\n",
    "    node_exp = camex.get_explanation_node(node_idx = node_idx, x = data.x, edge_index = data.edge_index, y = data.y)\n",
    "\n",
    "    imp_nodes = []\n",
    "\n",
    "    mask = torch.sigmoid(node_exp.node_imp) >= 0.5\n",
    "\n",
    "    for k in node_exp.node_reference.keys():\n",
    "\n",
    "        if mask[node_exp.node_reference[k]].item() == 1:\n",
    "        \n",
    "            imp_nodes.append(k)\n",
    "\n",
    "    return imp_nodes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DIG_GXAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a96d9a9b024afc8122ecb662ff5349e8dc2ebea5f61559fdd75ed5db623e2825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
